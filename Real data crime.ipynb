{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMUNITIES AND CRIME dataset experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Getting imports and definitions\n",
    "#### (libraries and functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "\"\"\" \n",
    "SS GP algorithm functions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/hughw/Documents/MSC project/GP algorithms/Master function files')\n",
    "from GP_funcs_ZTMFSS import kernel_funcs\n",
    "from GP_funcs_ZTMFSS import model_funcs\n",
    "from GP_funcs_ZTMFSS import draw_GP\n",
    "from GP_funcs_ZTMFSS import fit\n",
    "from GP_funcs_ZTMFSS import diagnostics\n",
    "from GP_funcs_ZTMFSS import simulations\n",
    "from functools import partial\n",
    "os.chdir('C:/Users/hughw/Documents/MSC project/Real data')\n",
    "\n",
    "\"\"\"\n",
    "Importing in rpy2\n",
    "\"\"\"\n",
    "\n",
    "os.environ['R_USER'] = 'D:\\Anaconda3\\Lib\\site-packages\\rpy2'\n",
    "import rpy2\n",
    "print(rpy2.__version__)\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "# import R's \"base\" package\n",
    "base = importr('base')\n",
    "base.R_home()\n",
    "# import R's \"utils\" package\n",
    "utils = importr('utils')\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "# import R's utility package\n",
    "utils = rpackages.importr('utils')\n",
    "\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "\n",
    "# Function definitions for comparators - MAKE IT SO THEY ALWAYS JUST RETURN PREDICTIONS AND CLASSIFICATIONS\n",
    "\"\"\"\n",
    "Importing in requirements for SVGP Pytorch\n",
    "\"\"\"\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "\"\"\"\n",
    "Defining procedure to run SVGP\n",
    "\"\"\"\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def SVGP_train(y, X, lengthscale_init = 10, num_inducing=100, epochs=100, batch_size=100, learn_rate_variational = 0.1, learn_rate_hyper = 0.01, tol = 1e-4,seed=0, min_epochs = 100\n",
    "              , alpha = 0.05, print_=False):\n",
    "    \n",
    "    # setting dimensions\n",
    "    ntrain,ntest, p = len(y), len(X), len(X.T)\n",
    "    \n",
    "    # loading in data\n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # Creating model\n",
    "    class GPModel(ApproximateGP):\n",
    "        def __init__(self, inducing_points):\n",
    "            variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "            variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "            super(GPModel, self).__init__(variational_strategy)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel=gpytorch.kernels.RBFKernel(ard_num_dims = p))\n",
    "\n",
    "            # Initialize lengthscale\n",
    "            if np.any(lengthscale_init):\n",
    "                self.covar_module.base_kernel.lengthscale = np.ones(p)*lengthscale_init\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    np.random.seed(seed)\n",
    "    inducing_points = X[np.random.choice(ntrain,num_inducing,False), :]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "        num_epochs = epochs\n",
    "\n",
    "    # Setting up model training\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=learn_rate_variational)\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=learn_rate_hyper)\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    # Setting up convergence criteria\n",
    "    epochs_iter = tqdm.notebook.tqdm(range(epochs), desc=\"Epoch\")\n",
    "    i = 0\n",
    "    param_diff=1\n",
    "    loss_diff=1\n",
    "    param = 1/model.covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    # Training model\n",
    "    while (i < epochs and loss_diff>0) or i<min_epochs:\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            \n",
    "            ### Perform NGD step to optimize variational parameters\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "\n",
    "            ### Perform Adam step to optimize hyperparameters\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            hyperparameter_optimizer.step()\n",
    "        \n",
    "        # Update convergence criteria\n",
    "        i+=1\n",
    "        param_old = param\n",
    "        param = 1/model.covar_module.base_kernel.lengthscale\n",
    "        param_diff = np.mean(np.abs((param-param_old).detach().numpy()))\n",
    "        if i==1:\n",
    "            loss_new = loss.item()\n",
    "        else:\n",
    "            loss_old = loss_new\n",
    "            loss_new = loss.item()*alpha+(1-alpha)*loss_old\n",
    "            loss_diff = loss_old - loss_new\n",
    "        \n",
    "        numprint = min(10,p)\n",
    "        if print_:\n",
    "            print(np.sort(model.covar_module.base_kernel.lengthscale.detach().numpy())[0][:numprint])\n",
    "            print(loss_new, param_diff)\n",
    "    print(\"Runtime is \", time.time()-t)\n",
    "    \n",
    "    return model, loss, likelihood\n",
    "\n",
    "def SVGP_test(model,likelihood, ytest, Xtest, batch_size=100):\n",
    "\n",
    "    test_dataset = TensorDataset(Xtest, ytest)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    # Getting model evaluations\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    \n",
    "    print('Test MSE: {}'.format(torch.mean(torch.abs(means - ytest.cpu())**2)))\n",
    "    \n",
    "    return means\n",
    "\n",
    "\"\"\"\n",
    "Defining procedure to run SGP\n",
    "\"\"\"\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "def SGP_train(y, X, lengthscale_init = 10, num_inducing=100, iterations=100, learn_rate = 0.1, tol = 1e-4, seed=0, min_iterations = 100, alpha = 0.05, print_=False):\n",
    "    \n",
    "    # setting dimensions\n",
    "    ntrain,ntest, p = len(y), len(X), len(X.T)\n",
    "\n",
    "    # Creating model\n",
    "    \n",
    "    class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, inducing_points):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = ConstantMean()\n",
    "            self.base_covar_module = ScaleKernel(RBFKernel(ard_num_dims = p))\n",
    "            self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=inducing_points, likelihood=likelihood)\n",
    "\n",
    "            # Initialize lengthscale\n",
    "            if np.any(lengthscale_init):\n",
    "                self.base_covar_module.base_kernel.lengthscale = np.ones(p)*lengthscale_init\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    inducing_points = X[np.random.choice(ntrain,num_inducing,False), :]\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood, inducing_points)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    # Setting up model training\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed) \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    # Setting up convergence criteria\n",
    "    i = 0\n",
    "    param_diff=1\n",
    "    loss_diff=1\n",
    "    param = 1/model.base_covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    # Training model\n",
    "    while (i < iterations and loss_diff>0) or i<min_iterations:\n",
    "        \n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update convergence criteria\n",
    "        i+=1\n",
    "        param_old = param\n",
    "        param = 1/model.base_covar_module.base_kernel.lengthscale\n",
    "        param_diff = np.mean(np.abs((param-param_old).detach().numpy()))\n",
    "        if i==1:\n",
    "            loss_new = loss.item()\n",
    "        else:\n",
    "            loss_old = loss_new\n",
    "            loss_new = loss.item()*alpha+(1-alpha)*loss_old\n",
    "            loss_diff = loss_old - loss_new\n",
    "        \n",
    "        numprint = min(10,p)\n",
    "        if not i % 1:\n",
    "            if print_:\n",
    "                print(np.sort(model.base_covar_module.base_kernel.lengthscale.detach().numpy())[0][:numprint])\n",
    "                print(loss_new, param_diff)\n",
    "                print('Iter %d - Loss: %.3f' % (i + 1, loss.item()))\n",
    "    print(\"Runtime is \", time.time()-t)\n",
    "    \n",
    "    return model, loss, likelihood\n",
    "\n",
    "def SGP_test(model,likelihood, ytest, Xtest):\n",
    "    \n",
    "    # Getting model evaluations\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with gpytorch.settings.max_preconditioner_size(10), torch.no_grad():\n",
    "        preds = model(Xtest)\n",
    "    \n",
    "    print('Test MSE: {}'.format(torch.mean(torch.abs(preds.mean - ytest.cpu())**2)))\n",
    "    \n",
    "    return preds.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Getting data \n",
    "#### (takes in a file and outputs cleaned data y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputting raw data\n",
    "from matplotlib import rcParams, rc_file_defaults\n",
    "rc_file_defaults()\n",
    "plt.rc('axes',edgecolor='black')\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams.update({'text.color' : \"black\",\n",
    "                      'xtick.color' : \"black\",\n",
    "                      'ytick.color' : \"black\",\n",
    "                     'axes.labelcolor' : \"black\"})\n",
    "\n",
    "df = pd. read_csv (\"communities.txt\", sep=\",\") \n",
    "data = np.array(df)\n",
    "\n",
    "# Box cox transformations to dependent\n",
    "y = data[:,len(data.T)-1].astype(float)\n",
    "plt.hist(y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(311)\n",
    "prob = stats.probplot(y, dist=stats.norm, plot=ax1)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_title('Probplot against normal distribution')\n",
    "\n",
    "ax2 = fig.add_subplot(312)\n",
    "yt, _ = stats.boxcox(y+1e-10)\n",
    "prob = stats.probplot(yt, dist=stats.norm, plot=ax2)\n",
    "ax2.set_title('Probplot after Box-Cox transformation')\n",
    "plt.show()\n",
    "\n",
    "# Counting NAs and removing NA columns \n",
    "NA_ind = data[:,5:]=='?'\n",
    "NA_rowcount = np.sum(NA_ind, 0)\n",
    "D = data[:,5:][:,NA_rowcount==0].astype(float)\n",
    "print(\"dimensions are : \", np.shape(D))\n",
    "\n",
    "# Getting correlations\n",
    "n,p = np.shape(D)\n",
    "d = p-1\n",
    "corrxy = np.zeros(p)\n",
    "for i in range(len(D.T)-1):\n",
    "     corrxy[i]=np.corrcoef(D[:,i],yt)[0,1]\n",
    "sns.set(font_scale = 3)\n",
    "fig,axs = plt.subplots(figsize = (25,20))\n",
    "fig.set_facecolor('white')\n",
    "axs.set_facecolor('white')\n",
    "plt.bar(range(p),height = corrxy, color = \"blue\")\n",
    "plt.title(\"Correlation between y, X\")\n",
    "plt.show()\n",
    "fig.savefig(\"CorryX_communities\")\n",
    "fig,axs = plt.subplots(figsize = (15,10))\n",
    "sns.heatmap(np.abs(np.corrcoef(D[:,:len(D.T)-1].T)), label = \"Correlation matrix\", yticklabels = False, xticklabels = False, color = \"black\")\n",
    "plt.title(r\"Communities and crime $\\rho_{XX}$\", fontsize = 50, color = \"black\")\n",
    "fig.savefig(\"CorrX_communities\")\n",
    "\n",
    "# get y, X\n",
    "y = yt\n",
    "X = D[:,:len(D.T)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Setting up experiment dimensions \n",
    "#### (determine random train:test splits and set up storage objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:test splitting settings\n",
    "n_replications = 10\n",
    "ntrain = int(0.8*n)\n",
    "ntest = n - ntrain\n",
    "nmodel = 3\n",
    "minibatch = 256\n",
    "nns=256\n",
    "kern = kernel_funcs.gaussian\n",
    "grad_kern = kernel_funcs.grad_gaussian\n",
    "post_var = False\n",
    "post_cov = False\n",
    "train_largest = True\n",
    "MC=1000\n",
    "reg = 0.01\n",
    "v_l0, v_l1, a,b = 1e+4,1e-4,1e-3,1e-3\n",
    "ELBO_sample=1000\n",
    "temp=1\n",
    "scale_vals = 2**np.linspace(np.log2(100),-np.log2(100),11)\n",
    "\n",
    "# Storage objects\n",
    "MSE = np.zeros((n_replications, nmodel))\n",
    "Runtime = np.zeros((n_replications, nmodel))\n",
    "L = np.zeros((n_replications, nmodel, d))\n",
    "Lambda = np.zeros((n_replications, nmodel, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Iterating over train:test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BMA over different minibatch sizes\n",
    "\"\"\"\n",
    "i=0\n",
    "j=0\n",
    "while i < n_replications: # use just three runs as takes too long to do more!\n",
    "    np.random.seed(j)\n",
    "    j+=1\n",
    "    \n",
    "    shuffled_indexes = np.random.choice(n,n,False)\n",
    "    y_shuffle = y[shuffled_indexes]\n",
    "    X_shuffle = X[shuffled_indexes]\n",
    "    \n",
    "    if np.min(X_shuffle[:ntrain].var(0))>0:\n",
    "        \n",
    "            \n",
    "    # Get current train:test split\n",
    "        ytrain = ((y_shuffle[:ntrain]-y_shuffle[:ntrain].mean())/y_shuffle[:ntrain].var()**0.5).reshape(ntrain,1)\n",
    "        ytest = ((y_shuffle[ntrain:]-y_shuffle[:ntrain].mean())/y_shuffle[:ntrain].var()**0.5).reshape(ntest,1)\n",
    "\n",
    "        Xtrain = (X_shuffle[:ntrain] - X_shuffle[:ntrain].mean(0))/X_shuffle[:ntrain].var(0)**.5\n",
    "        Xtest = (X_shuffle[ntrain:] - X_shuffle[:ntrain].mean(0))/X_shuffle[:ntrain].var(0)**.5\n",
    "\n",
    "    # Run all SS-GP algorithms,  get predictions and store performance\n",
    "\n",
    "        \"\"\"\n",
    "        Initialising model weight vector\n",
    "        \"\"\"\n",
    "        weights = np.zeros(11)\n",
    "        \n",
    "        \"\"\"\n",
    "        BMA over X_scale (256)\n",
    "        \"\"\"\n",
    "        m =0\n",
    "        t = time.time()\n",
    "\n",
    "        # Running algorithm\n",
    "        testing_algorithm = partial(diagnostics.get_pred_posterior_GP_NN,reg = 0.01 ,kern = kern, grad_kern = grad_kern, latents = False, pred_selected = True, post_var = False, NN=nns, print_=True)\n",
    "        hyper_vals = [1e+4*scale_vals,1e-4*scale_vals, np.linspace(0,10,11).astype(int)]\n",
    "        hyper_arg = [\"v0\",\"v1\", \"seed\"]\n",
    "        best_pair, selections, losses, Results = fit.hyper_opt_SSGP(\n",
    "                                            ytrain, Xtrain, fit.VB_EM_GP_SS, testing_algorithm, hyper_arg, hyper_vals, method =  \"ML\", metric = \"elbo\", \n",
    "                                            training_args=[\"final_ELBO_sample\", \"seed\", \"iter_remove\", \"print_VBEM\", \"learn_rate\", \"subsample\", \"sampling_strat\", \"min_VBEM_iter\",\"init_GP_iter\", \"max_VBEM_iter\", \"GP_fit_tol\", \"VBEM_tol\"], \n",
    "                                            training_arg_vals=[0, i, True, False, 0.0025, 256, \"nn\" ,5, 100,  5 ,           1e-5,         0.1/d, 1])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # MSE WEIGHTING\n",
    "        \n",
    "        log_predictives = np.zeros(len(Results))\n",
    "        for k in np.where(selections!=0)[0]:\n",
    "            log_predictives[k] =  diagnostics.get_pred_posterior_GP_NN_CV(ytrain,Xtrain,Results[j],0.01,kern,NN=64, fraction=n**-0.5,post_var=True, print_=False, use_tree=False, leaf_size=100, seed=0)\n",
    "            print(k)\n",
    "        log_predictives[np.where(selections==0)[0]]=np.min(log_predictives)-1000\n",
    "        \n",
    "        train_time = time.time()-t\n",
    "\n",
    "        min_loss = np.max(log_predictives)\n",
    "        weights = np.exp(log_predictives - min_loss)*(np.abs(log_predictives - min_loss)<=500)\n",
    "        weights = weights/weights.sum()\n",
    "\n",
    "        Lmbda = np.zeros((len(Results), d))\n",
    "        Ls = np.zeros((len(Results), d))\n",
    "        for k in range(len(Results)):\n",
    "            Lmbda[k] = Results[k][3]\n",
    "            Ls[k] = np.abs(Results[k][0][0])         \n",
    "        PIP = Lmbda.T @ weights\n",
    "        l = Ls.T @ weights\n",
    "\n",
    "        BMA_preds = diagnostics.get_BMA_predictions(ytrain,Xtrain,Xtest,testing_algorithm, Results,weights, MC_samples=MC)\n",
    "        MSE[i,m] = simulations.MSE_pc(BMA_preds[1],ytest)\n",
    "        Runtime[i,m] = time.time()-t\n",
    "        L[i,m,:] = l\n",
    "        Lambda[i,m,:] = PIP\n",
    "        print(\"MSE is : \", MSE[i,m])\n",
    "        print(\"Runtime is : \", Runtime[i,m])\n",
    "        \n",
    "        \"\"\"\n",
    "        BMA over X_scale (128)\n",
    "        \"\"\"\n",
    "        m += 1 \n",
    "        t = time.time()\n",
    "\n",
    "        # Running algorithm\n",
    "        hyper_vals = [1e+4*scale_vals,1e-4*scale_vals, np.linspace(0,10,11).astype(int)]\n",
    "        hyper_arg = [\"v0\",\"v1\", \"seed\"]\n",
    "        best_pair, selections, losses, Results = fit.hyper_opt_SSGP(\n",
    "                                            ytrain, Xtrain, fit.VB_EM_GP_SS, testing_algorithm, hyper_arg, hyper_vals, method =  \"ML\", metric = \"elbo\", \n",
    "                                            training_args=[\"final_ELBO_sample\", \"seed\", \"iter_remove\", \"print_VBEM\", \"learn_rate\", \"subsample\", \"sampling_strat\", \"min_VBEM_iter\",\"init_GP_iter\", \"max_VBEM_iter\", \"GP_fit_tol\", \"VBEM_tol\"], \n",
    "                                            training_arg_vals=[0, i, True, False, 0.005, 128, \"nn\" ,5, 100,  5 ,           1e-5,         0.1/d, 1])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        log_predictives = np.zeros(len(Results))\n",
    "        for k in np.where(selections!=0)[0]:\n",
    "            log_predictives[k] =  diagnostics.get_pred_posterior_GP_NN_CV(ytrain,Xtrain,Results[j],0.01,kern,NN=64, fraction=n**-0.5,post_var=True, print_=False, use_tree=False, leaf_size=100, seed=0)\n",
    "            print(k)\n",
    "        log_predictives[np.where(selections==0)[0]]=np.min(log_predictives)-1000 \n",
    "        \n",
    "        train_time = time.time()-t\n",
    "\n",
    "        min_loss = np.max(log_predictives)\n",
    "        weights = np.exp(log_predictives - min_loss)*(np.abs(log_predictives - min_loss)<=500)\n",
    "        weights = weights/weights.sum()\n",
    "\n",
    "        Lmbda = np.zeros((len(Results), d))\n",
    "        Ls = np.zeros((len(Results), d))\n",
    "        for k in range(len(Results)):\n",
    "            Lmbda[k] = Results[k][3]\n",
    "            Ls[k] = np.abs(Results[k][0][0])         \n",
    "        PIP = Lmbda.T @ weights\n",
    "        l = Ls.T @ weights\n",
    "\n",
    "        BMA_preds = diagnostics.get_BMA_predictions(ytrain,Xtrain,Xtest,testing_algorithm, Results,weights, MC_samples=MC)\n",
    "        MSE[i,m] = simulations.MSE_pc(BMA_preds[1],ytest)\n",
    "        Runtime[i,m] = time.time()-t\n",
    "        L[i,m,:] = l\n",
    "        Lambda[i,m,:] = PIP\n",
    "        print(\"MSE is : \", MSE[i,m])\n",
    "        print(\"Runtime is : \", Runtime[i,m])\n",
    "        \n",
    "        \"\"\"\n",
    "        BMA over X_scale (64)\n",
    "        \"\"\"\n",
    "        m +=1\n",
    "        t = time.time()\n",
    "\n",
    "        # Running algorithm\n",
    "        hyper_vals = [1e+4*scale_vals,1e-4*scale_vals, np.linspace(0,10,11).astype(int)]\n",
    "        hyper_arg = [\"v0\",\"v1\", \"seed\"]\n",
    "        best_pair, selections, losses, Results = fit.hyper_opt_SSGP(\n",
    "                                            ytrain, Xtrain, fit.VB_EM_GP_SS, testing_algorithm, hyper_arg, hyper_vals, method =  \"ML\", metric = \"elbo\", \n",
    "                                            training_args=[\"final_ELBO_sample\", \"seed\", \"iter_remove\", \"print_VBEM\", \"learn_rate\", \"subsample\", \"sampling_strat\", \"min_VBEM_iter\",\"init_GP_iter\", \"max_VBEM_iter\", \"GP_fit_tol\", \"VBEM_tol\"], \n",
    "                                            training_arg_vals=[0, i, True, False, 0.01, 64, \"nn\" ,5, 100,  5 ,           1e-5,         0.1/d, 1])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        log_predictives = np.zeros(len(Results))\n",
    "        for k in np.where(selections!=0)[0]:\n",
    "            log_predictives[k] =  diagnostics.get_pred_posterior_GP_NN_CV(ytrain,Xtrain,Results[j],0.01,kern,NN=64, fraction=n**-0.5,post_var=True, print_=False, use_tree=False, leaf_size=100, seed=0)\n",
    "            print(k)\n",
    "        log_predictives[np.where(selections==0)[0]]=np.min(log_predictives)-1000    \n",
    "        train_time = time.time()-t\n",
    "\n",
    "        min_loss = np.max(log_predictives)\n",
    "        weights = np.exp(log_predictives - min_loss)*(np.abs(log_predictives - min_loss)<=500)\n",
    "        weights = weights/weights.sum()\n",
    "\n",
    "        Lmbda = np.zeros((len(Results), d))\n",
    "        Ls = np.zeros((len(Results), d))\n",
    "        for k in range(len(Results)):\n",
    "            Lmbda[k] = Results[k][3]\n",
    "            Ls[k] = np.abs(Results[k][0][0])         \n",
    "        PIP = Lmbda.T @ weights\n",
    "        l = Ls.T @ weights\n",
    "\n",
    "        BMA_preds = diagnostics.get_BMA_predictions(ytrain,Xtrain,Xtest,testing_algorithm, Results,weights, MC_samples=MC)\n",
    "        MSE[i,m] = simulations.MSE_pc(BMA_preds[1],ytest)\n",
    "        Runtime[i,m] = time.time()-t\n",
    "        L[i,m,:] = l\n",
    "        Lambda[i,m,:] = PIP\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"MSE is : \", MSE[i,m])\n",
    "        print(\"Runtime is : \", Runtime[i,m])\n",
    "          \n",
    "        print(\"MSE mean is : \", MSE[:i+1].mean(0))\n",
    "        print(\"Runtime mean is : \", Runtime[:i+1].mean(0))\n",
    "        \n",
    "        i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs for SVGP/SGP only\n",
    "\"\"\"\n",
    "i=0\n",
    "j=0\n",
    "while i < n_replications: # use just three runs as takes too long to do more!\n",
    "    np.random.seed(j)\n",
    "    j+=1\n",
    "    \n",
    "    shuffled_indexes = np.random.choice(n,n,False)\n",
    "    y_shuffle = y[shuffled_indexes]\n",
    "    X_shuffle = X[shuffled_indexes]\n",
    "    \n",
    "    if np.min(X_shuffle[:ntrain].var(0))>0:\n",
    "            \n",
    "    # Get current train:test split\n",
    "        ytrain = ((y_shuffle[:ntrain]-y_shuffle[:ntrain].mean())/y_shuffle[:ntrain].var()**0.5).reshape(ntrain,1)\n",
    "        ytest = ((y_shuffle[ntrain:]-y_shuffle[:ntrain].mean())/y_shuffle[:ntrain].var()**0.5).reshape(ntest,1)\n",
    "\n",
    "        Xtrain = (X_shuffle[:ntrain] - X_shuffle[:ntrain].mean(0))/X_shuffle[:ntrain].var(0)**.5\n",
    "        Xtest = (X_shuffle[ntrain:] - X_shuffle[:ntrain].mean(0))/X_shuffle[:ntrain].var(0)**.5\n",
    "\n",
    "       \n",
    "        \"\"\"\n",
    "        SGP\n",
    "        \"\"\"\n",
    "        m += 1\n",
    "        Ys = ((y_shuffle-y_shuffle[:ntrain].mean())/y_shuffle[:ntrain].var()**0.5).reshape(n,1)\n",
    "        Xs = ((X_shuffle-X_shuffle[:ntrain].mean(0))/X_shuffle[:ntrain].var(0)**0.5).reshape(n,d)\n",
    "        Xtorch = torch.from_numpy(Xs).float()\n",
    "        Ytorch = torch.from_numpy(Ys.reshape(ntrain+ntest,)).float()\n",
    "\n",
    "\n",
    "        train_x = Xtorch[:ntrain, :]\n",
    "        train_y = Ytorch[:ntrain]\n",
    "        test_x = Xtorch[ntrain:, :]\n",
    "        test_y = Ytorch[ntrain:]\n",
    "\n",
    "        t = time.time()\n",
    "        model,loss,likelihood = SGP_train(train_y, train_x, lengthscale_init = [], num_inducing=512, iterations=100, learn_rate = 0.1, \n",
    "                                    seed = 0, min_iterations=100, alpha = 0.01)\n",
    "        preds = SVGP_test(model,likelihood,test_y, test_x, batch_size=512)\n",
    "        Runtime[i,m] = time.time()-t\n",
    "        MSE[i,m] = simulations.MSE_pc(np.array(preds),np.array(test_y))\n",
    "        print(\"MSE is : \", MSE[i,m])\n",
    "        print(\"Runtime is : \", Runtime[i,m])\n",
    "        L[i,m,:] = 1/model.covar_module.base_kernel.lengthscale.detach().numpy()[0]\n",
    "\n",
    "        \"\"\"\n",
    "        SVGP\n",
    "        \"\"\"\n",
    "        m += 1\n",
    "        model,loss,likelihood = SVGP_train(train_y, train_x, lengthscale_init = [], num_inducing=1024, epochs=100, batch_size=1024, learn_rate_variational = 0.01, \n",
    "                                           learn_rate_hyper = 0.01, tol = 1e-3, seed = 0, min_epochs = 100, alpha = 0.01)\n",
    "\n",
    "        preds = SVGP_test(model,likelihood,test_y, test_x, batch_size=1024)\n",
    "        Runtime[i,m] = time.time()-t\n",
    "        MSE[i,m] = simulations.MSE_pc(np.array(preds),np.array(test_y))\n",
    "        L[i,m,:] = 1/model.covar_module.base_kernel.lengthscale.detach().numpy()[0]\n",
    "        print(\"MSE is : \", MSE[i,m])\n",
    "        print(\"Runtime is : \", Runtime[i,m])\n",
    "\n",
    "        print(\"MSE mean is : \", MSE[:i+1].mean(0))\n",
    "        print(\"Runtime mean is : \", Runtime[:i+1].mean(0))\n",
    "        \n",
    "        i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/hughw/Documents/MSC project/Real data/Communities and crime')\n",
    "from datetime import date\n",
    "Output = {\"L\" : L[:10], \"Lambda\" : Lambda[:10],\"MSE\" : MSE[:10], \"Runtime\" : Runtime[:10]}\n",
    "String = \"EXPERIMENT_{0}_Communities and crime 10 trials\".format(\n",
    "    date.today())\n",
    "np.save(String, Output) # saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
