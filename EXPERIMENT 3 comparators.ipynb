{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing in requirements for SVGP Pytorch\n",
    "\"\"\"\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "\"\"\"\n",
    "Importing in libraries for SGD-SS-GP\n",
    "\"\"\"\n",
    "# Requirements for algorithms\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "Importing algorithm functions\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('C:/Users/hughw/Documents/MSC project/GP algorithms/Master function files')\n",
    "from GP_funcs_ZTMFSS import kernel_funcs\n",
    "from GP_funcs_ZTMFSS import model_funcs\n",
    "from GP_funcs_ZTMFSS import draw_GP\n",
    "from GP_funcs_ZTMFSS import fit\n",
    "from GP_funcs_ZTMFSS import diagnostics\n",
    "from GP_funcs_ZTMFSS import simulations\n",
    "from functools import partial\n",
    "os.chdir('C:/Users/hughw/Documents/MSC project/Simulation results')\n",
    "\n",
    "\"\"\"\n",
    "Defining procedure to run SVGP\n",
    "\"\"\"\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def SVGP_train(y, X, lengthscale_init = 1, num_inducing=100, epochs=100, batch_size=100, learn_rate_variational = 0.1, learn_rate_hyper = 0.01, tol = 1e-4,seed=0, min_epochs = 100\n",
    "              , alpha = 0.05):\n",
    "    \n",
    "    # setting dimensions\n",
    "    ntrain,ntest, p = len(y), len(X), len(X.T)\n",
    "    \n",
    "    # loading in data\n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    losses = np.zeros(1)\n",
    "\n",
    "    # Creating model\n",
    "    class GPModel(ApproximateGP):\n",
    "        def __init__(self, inducing_points):\n",
    "            variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "            variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "            super(GPModel, self).__init__(variational_strategy)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel=gpytorch.kernels.RBFKernel(ard_num_dims = p))\n",
    "\n",
    "            # Initialize lengthscale\n",
    "            if np.any(lengthscale_init):\n",
    "                self.covar_module.base_kernel.lengthscale *= lengthscale_init\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    np.random.seed(seed)\n",
    "    inducing_points = X[np.random.choice(ntrain,num_inducing,False), :]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "        num_epochs = epochs\n",
    "\n",
    "    # Setting up model training\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=learn_rate_variational)\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=learn_rate_hyper)\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    # Setting up convergence criteria\n",
    "    epochs_iter = tqdm.notebook.tqdm(range(epochs), desc=\"Epoch\")\n",
    "    i = 0\n",
    "    param_diff=1\n",
    "    loss_diff=1\n",
    "    param = 1/model.covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    # Training model\n",
    "    while (i < epochs and loss_diff>0) or i<min_epochs:\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            \n",
    "            ### Perform NGD step to optimize variational parameters\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "\n",
    "            ### Perform Adam step to optimize hyperparameters\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            hyperparameter_optimizer.step()\n",
    "        \n",
    "        # Update convergence criteria\n",
    "        i+=1\n",
    "        param_old = param\n",
    "        param = 1/model.covar_module.base_kernel.lengthscale\n",
    "        param_diff = np.mean(np.abs((param-param_old).detach().numpy()))\n",
    "        if i==1:\n",
    "            loss_new = loss.item()\n",
    "        else:\n",
    "            loss_old = loss_new\n",
    "            loss_new = loss.item()*alpha+(1-alpha)*loss_old\n",
    "            loss_diff = loss_old - loss_new\n",
    "        \n",
    "        losses = np.append(losses,loss.item())\n",
    "        \n",
    "        numprint = min(10,p)\n",
    "        print(model.covar_module.base_kernel.lengthscale.detach().numpy()[0][:numprint])\n",
    "        print(loss_new, param_diff)\n",
    "    print(\"Runtime is \", time.time()-t)\n",
    "    \n",
    "    return model, loss, likelihood, losses\n",
    "\n",
    "def SVGP_test(model,likelihood, ytest, Xtest, batch_size=100):\n",
    "\n",
    "    test_dataset = TensorDataset(Xtest, ytest)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    # Getting model evaluations\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    \n",
    "    print('Test MSE: {}'.format(torch.mean(torch.abs(means - ytest.cpu())**2)))\n",
    "    \n",
    "    return means\n",
    "\n",
    "\"\"\"\n",
    "Defining procedure to run SGP\n",
    "\"\"\"\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "def SGP_train(y, X, lengthscale_init = 1, num_inducing=100, iterations=100, learn_rate = 0.1, tol = 1e-4, seed=0, min_iterations = 100, alpha = 0.05):\n",
    "    \n",
    "    # setting dimensions\n",
    "    ntrain,ntest, p = len(y), len(X), len(X.T)\n",
    "    \n",
    "    losses = np.zeros(1)\n",
    "\n",
    "    # Creating model\n",
    "    \n",
    "    class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, inducing_points):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = ConstantMean()\n",
    "            self.base_covar_module = ScaleKernel(RBFKernel(ard_num_dims = p))\n",
    "            self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=inducing_points, likelihood=likelihood)\n",
    "\n",
    "            # Initialize lengthscale\n",
    "            if np.any(lengthscale_init):\n",
    "                self.base_covar_module.base_kernel.lengthscale *= lengthscale_init\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    inducing_points = X[np.random.choice(ntrain,num_inducing,False), :]\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood, inducing_points)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    # Setting up model training\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed) \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    # Setting up convergence criteria\n",
    "    i = 0\n",
    "    param_diff=1\n",
    "    loss_diff=1\n",
    "    param = 1/model.base_covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    # Training model\n",
    "    while (i < iterations and loss_diff>0) or i<min_iterations:\n",
    "        \n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update convergence criteria\n",
    "        i+=1\n",
    "        param_old = param\n",
    "        param = 1/model.base_covar_module.base_kernel.lengthscale\n",
    "        param_diff = np.mean(np.abs((param-param_old).detach().numpy()))\n",
    "        if i==1:\n",
    "            loss_new = loss.item()\n",
    "        else:\n",
    "            loss_old = loss_new\n",
    "            loss_new = loss.item()*alpha+(1-alpha)*loss_old\n",
    "            loss_diff = loss_old - loss_new\n",
    "        \n",
    "        losses = np.append(losses, loss.item())\n",
    "        \n",
    "        numprint = min(10,p)\n",
    "        if not i % 1:\n",
    "            print(model.base_covar_module.base_kernel.lengthscale.detach().numpy()[0][:numprint])\n",
    "            print(loss_new, param_diff)\n",
    "            print('Iter %d - Loss: %.3f' % (i + 1, loss.item()))\n",
    "    print(\"Runtime is \", time.time()-t)\n",
    "    \n",
    "    return model, loss, likelihood, losses\n",
    "\n",
    "def SGP_test(model,likelihood, ytest, Xtest):\n",
    "    \n",
    "    # Getting model evaluations\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with gpytorch.settings.max_preconditioner_size(10), torch.no_grad():\n",
    "        preds = model(Xtest)\n",
    "    \n",
    "    print('Test MSE: {}'.format(torch.mean(torch.abs(preds.mean - ytest.cpu())**2)))\n",
    "    \n",
    "    return preds.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulation settings\n",
    "\"\"\"\n",
    "dlist = [10, 100, 1000, 10000] # toggle\n",
    "post_fit_svgp = False\n",
    "batch=1024\n",
    "minibatch=100\n",
    "inducing=512\n",
    "NNpred=True\n",
    "nns=100\n",
    "epochs = [100,100,300,300]\n",
    "iters = [100,100,300,300]\n",
    "ntrial = 1\n",
    "\n",
    "modelrun = [\"SGP\", \"SVGP\"]\n",
    "\n",
    "\"\"\"\n",
    "Setting up dimensions and objects for drawing data\n",
    "\"\"\"\n",
    "n = 10000 # toggle\n",
    "ntrain = n\n",
    "ntest = 10000\n",
    "q=2\n",
    "corrzz=0.5\n",
    "corrxz=0.5\n",
    "r2=0.75\n",
    "lin = False\n",
    "block_corr = False\n",
    "\"\"\"\n",
    "Objects to store results\n",
    "\"\"\"\n",
    "names = [\"SGP\", \"SVGP\"]\n",
    "nmodel = len(names)\n",
    "Lengthscales = []\n",
    "Lambdas = []\n",
    "Predictions = []\n",
    "MSerrors_Y = []\n",
    "MSerrors_F = []\n",
    "Training_times = []\n",
    "Testing_times = []\n",
    "Xtestvals = []\n",
    "Ytestvals = []\n",
    "Ftestvals = []\n",
    "Loss = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Control panel toggle for training/testing etc.\n",
    "\"\"\"\n",
    "train = True\n",
    "test = True\n",
    "test_store = True\n",
    "plot = True\n",
    "train_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dlist)):\n",
    "    \n",
    "    p=dlist[i]\n",
    "    np.random.seed(8750)\n",
    "    \n",
    "    L = np.zeros((nmodel,ntrial,p))\n",
    "    Lambda = np.zeros((nmodel,ntrial,p))\n",
    "    Preds = np.zeros((nmodel,ntrial,ntest))\n",
    "    MSE_F = np.zeros((nmodel, ntrial))\n",
    "    MSE_Y = np.zeros((nmodel, ntrial))\n",
    "    Train_time = np.zeros((nmodel, ntrial))\n",
    "    Test_time = np.zeros((nmodel, ntrial))\n",
    "    TPR = np.zeros((nmodel, ntrial))\n",
    "    PPV = np.zeros((nmodel, ntrial))\n",
    "    Losses = np.zeros((nmodel, ntrial,np.max((epochs[i],iters[i]))+1))\n",
    "\n",
    "    \"\"\"\n",
    "    Looping over iterations of data draws\n",
    "    \"\"\"\n",
    "                   \n",
    "    for j in range(ntrial):\n",
    "        np.random.seed(j) # Setting seed to draw data\n",
    "                   \n",
    "        \"\"\"\n",
    "        Drawing data to use for simulation\n",
    "        \"\"\"\n",
    "        Y,F,X,e,sigma,select,ntrain,ntest = draw_GP.draw_parametric_sin_2d_new2(n, ntest, p-2, 0, 1, corrxz=corrxz,corrzz=corrzz, r2=r2, lin=lin, block_corr = block_corr)\n",
    "\n",
    "        X = (X-X[:ntrain].mean(0))/X[:ntrain].var(0)**0.5\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(Y.reshape(ntrain+ntest,)).float()\n",
    "        f = torch.from_numpy(F.reshape(ntrain+ntest,)).float()\n",
    "\n",
    "        train_x = X[:ntrain, :]\n",
    "        train_y = y[:ntrain]\n",
    "        test_x = X[ntrain:, :]\n",
    "        test_y = y[ntrain:]\n",
    "        test_f = f[ntrain:]\n",
    "\n",
    "        Xtrain = np.array(train_x)\n",
    "        ytrain =np.array(train_y).reshape(len(train_y),1)\n",
    "        Xtest = np.array(test_x)\n",
    "        ytest = np.array(test_y).reshape(len(test_y),1)\n",
    "        ftest = np.array(test_f).reshape(len(test_f),1)\n",
    "\n",
    "        # Storing relevant X's and y,f for test set\n",
    "        Xtestvals.append(Xtest[:,:q])\n",
    "        Ytestvals.append(ytest)\n",
    "        Ftestvals.append(ftest)\n",
    "\n",
    "        \"\"\"\n",
    "        Running SGP\n",
    "        \"\"\"\n",
    "        if \"SGP\" in modelrun:\n",
    "            # Training\n",
    "            if train:\n",
    "                t=time.time()\n",
    "                model,loss,likelihood, losses = SGP_train(train_y, train_x, lengthscale_init = 1 +9*(p>100)+20*(p>1000), num_inducing=inducing, iterations=iters[i], learn_rate = 0.1, \n",
    "                                                seed = train_seed, min_iterations=iters[i], alpha = 0.01)\n",
    "                Train_time[0,j] = time.time()-t\n",
    "                L[0,j] = 1/model.base_covar_module.base_kernel.lengthscale.detach().numpy()\n",
    "\n",
    "            # Testing\n",
    "            if test:\n",
    "                t = time.time()\n",
    "                preds = SVGP_test(model,likelihood,test_y, test_x, batch_size=100)\n",
    "\n",
    "                if test_store:\n",
    "                    Test_time[0,j] = time.time()-t\n",
    "                    MSE_Y[0,j] = (torch.mean(torch.abs(preds - test_y.cpu())**2)).detach().numpy()\n",
    "                    MSE_F[0,j] = (torch.mean(torch.abs(preds - test_f.cpu())**2)).detach().numpy()\n",
    "                    Preds[0,j] = preds\n",
    "                    Losses[0,j,:iters[i]+1] = losses\n",
    "                    print(\"Latent test function MSE : \", MSE_F[0])\n",
    "                    print(\"Test time is : \", Test_time[0])\n",
    "\n",
    "            # Plotting results\n",
    "            if plot:\n",
    "                diagnostics.plot_length_scales_pip(np.concatenate((np.repeat(1,q),np.repeat(0,p-q))),L[0,j],L[0,j]>0.1)\n",
    "                ztrue = np.reshape(ftest,(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "                z = np.reshape(preds.detach().numpy(),(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "\n",
    "                x1 =  np.reshape(Xtest[:,0],(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "                x2 =  np.reshape(Xtest[:,1],(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "\n",
    "                fig,axs = plt.subplots(2,figsize = (10,15))\n",
    "                z_min, z_max = ztrue.min(), ztrue.max()\n",
    "                c = axs[0].pcolormesh(x1, x2, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "                axs[0].set_title('Predicted test surface')\n",
    "                axs[0].axis([x1.min(), x1.max(), x2.min(), x2.max()])\n",
    "                fig.colorbar(c, ax=axs[0])\n",
    "\n",
    "                z_min, z_max = ztrue.min(), ztrue.max()\n",
    "                c = axs[1].pcolormesh(x1, x2, ztrue, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "                axs[1].set_title('True test surface')\n",
    "                axs[1].axis([x1.min(), x1.max(), x2.min(), x2.max()])\n",
    "                fig.colorbar(c, ax=axs[1])\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Latent test function MSE : \",(torch.mean(torch.abs(preds - test_f.cpu())**2)).detach().numpy())\n",
    "                print(\"Observed test data  MSE : \",(torch.mean(torch.abs(preds - test_y.cpu())**2)).detach().numpy())\n",
    "                \n",
    "        if \"SVGP\" in modelrun:\n",
    "            \"\"\"\n",
    "            Running SVGP\n",
    "            \"\"\"\n",
    "\n",
    "            # Training\n",
    "            if train:\n",
    "                t=time.time()\n",
    "                model,loss,likelihood,losses = SVGP_train(train_y, train_x, lengthscale_init = 1+9*(p>100)+20*(p>1000), num_inducing=inducing, epochs=epochs[i], batch_size=batch, learn_rate_variational = 0.01, \n",
    "                                                   learn_rate_hyper = 0.01, tol = 1e-3, seed = train_seed, min_epochs = epochs[i], alpha = 0.01)\n",
    "                Train_time[1,j] = time.time()-t\n",
    "                L[1,j] = 1/model.covar_module.base_kernel.lengthscale.detach().numpy()\n",
    "\n",
    "            # Testing\n",
    "            if test:\n",
    "                t = time.time()\n",
    "                preds = SVGP_test(model,likelihood,test_y, test_x, batch_size=100)\n",
    "\n",
    "                if test_store:\n",
    "                    Test_time[1,j] = time.time()-t\n",
    "                    MSE_Y[1,j] = (torch.mean(torch.abs(preds - test_y.cpu())**2)).detach().numpy()\n",
    "                    MSE_F[1,j] = (torch.mean(torch.abs(preds - test_f.cpu())**2)).detach().numpy()\n",
    "                    Preds[1,j] = preds\n",
    "                    Losses[1,j,:epochs[i]+1] = losses\n",
    "                    print(\"Latent test function MSE : \", MSE_F[1])\n",
    "                    print(\"Test time is : \", Test_time[1])\n",
    "\n",
    "            # Plotting results\n",
    "            if plot:\n",
    "                diagnostics.plot_length_scales_pip(np.concatenate((np.repeat(1,q),np.repeat(0,p-q))),L[1,j],L[1,j]>0.1)\n",
    "                ztrue = np.reshape(ftest,(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "                z = np.reshape(preds.detach().numpy(),(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "\n",
    "                x1 =  np.reshape(Xtest[:,0],(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "                x2 =  np.reshape(Xtest[:,1],(int(len(ftest)**0.5),int(len(ftest)**0.5)))\n",
    "\n",
    "                fig,axs = plt.subplots(2,figsize = (10,15))\n",
    "                z_min, z_max = ztrue.min(), ztrue.max()\n",
    "                c = axs[0].pcolormesh(x1, x2, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "                axs[0].set_title('Predicted test surface')\n",
    "                axs[0].axis([x1.min(), x1.max(), x2.min(), x2.max()])\n",
    "                fig.colorbar(c, ax=axs[0])\n",
    "\n",
    "                z_min, z_max = ztrue.min(), ztrue.max()\n",
    "                c = axs[1].pcolormesh(x1, x2, ztrue, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "                axs[1].set_title('True test surface')\n",
    "                axs[1].axis([x1.min(), x1.max(), x2.min(), x2.max()])\n",
    "                fig.colorbar(c, ax=axs[1])\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Latent test function MSE : \",(torch.mean(torch.abs(preds - test_f.cpu())**2)).detach().numpy())\n",
    "                print(\"Observed test data  MSE : \",(torch.mean(torch.abs(preds - test_y.cpu())**2)).detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Storing results in master lists\n",
    "    \"\"\"\n",
    "\n",
    "    Lengthscales.append(L)\n",
    "    Lambdas.append(Lambda)\n",
    "    Predictions.append(Preds)\n",
    "    MSerrors_Y.append(MSE_Y)\n",
    "    MSerrors_F.append(MSE_F)\n",
    "    Training_times.append(Train_time)\n",
    "    Testing_times.append(Test_time)\n",
    "    Loss.append(Losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "Output = {\"Names\" : names, \"L\" : Lengthscales, \"Lambda\" : Lambdas,\"Preds\" : Predictions, \"MSE_Y\" : MSerrors_Y, \"MSE_F\" : MSerrors_F, \"Train_time\" : Training_times, \"Test_time\" : Testing_times}\n",
    "String = \"EXPERIMENT_{0}_2dsimupto100_{12}_n={1}_ntest={2}_pvals={3}_r2={11}_corr={4}_sigma2={5}_batch={6}_inducing={7}_sgdbatch={8}_nnpred={9}_nnpredsize={10}\".format(\n",
    "    date.today(),ntrain,ntest,dlist, corrxz, np.round(1-r2,1), batch, inducing,minibatch,NNpred,nns, r2, modelrun)\n",
    "np.save(String, Output) # saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
